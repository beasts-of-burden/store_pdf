from flask import Flask, render_template, request, jsonify
import torch
import os
import json
import csv
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, pipeline
from component.utils import ModelUtils
from component.template import template_dict
from component.splitter import DocumentSplitter
from sentence_transformers import SentenceTransformer
import copy
from werkzeug.utils import secure_filename
from component.generator import LocalLLMGenerator

# å®šä¹‰Documentç±»ç”¨äºè¡¨ç¤ºæ–‡æ¡£
class Document:
    def __init__(self, content, meta=None):
        self.content = content
        self.meta = meta if meta else {}

# å®šä¹‰DocumentStoreç±»ç”¨äºå­˜å‚¨æ–‡æ¡£
class InMemoryDocumentStore:
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.metadata = []
        self.doc_to_idx = {}
        self.next_idx = 0
    
    def write_documents(self, documents):
        for doc in documents:
            if isinstance(doc, dict):
                content = doc.get('content', '')
                embedding = doc.get('embedding')
                meta = doc.get('meta', {})
            else:
                content = getattr(doc, 'content', '')
                embedding = getattr(doc, 'embedding', None)
                meta = getattr(doc, 'meta', {})
            
            self.documents.append(content)
            self.embeddings.append(embedding)
            self.metadata.append(meta)
            self.doc_to_idx[self.next_idx] = len(self.documents) - 1
            self.next_idx += 1

# å®šä¹‰EmbeddingRetrieverç±»ç”¨äºæ£€ç´¢
class InMemoryEmbeddingRetriever:
    def __init__(self, document_store):
        self.document_store = document_store
    
    def run(self, query_embedding, top_k=3):
        if not self.document_store.embeddings or all(emb is None for emb in self.document_store.embeddings):
            return {'documents': []}
            
        similarities = []
        for idx, embedding in enumerate(self.document_store.embeddings):
            if embedding is not None:
                similarity = np.dot(query_embedding, embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(embedding))
                similarities.append((similarity, idx))
                
        similarities.sort(key=lambda x: x[0], reverse=True)
        top_indices = similarities[:top_k]
        
        retrieved_docs = []
        for sim, idx in top_indices:
            retrieved_docs.append({
                'content': self.document_store.documents[idx],
                'embedding': self.document_store.embeddings[idx],
                'meta': self.document_store.metadata[idx],
                'similarity': float(sim)
            })
            
        return {'documents': retrieved_docs}

# å®šä¹‰PromptBuilderç±»ç”¨äºæ„å»ºæç¤º
class PromptBuilder:
    def __init__(self, template, required_variables):
        self.template = template
        self.required_variables = required_variables
    
    def run(self, documents, query):
        # æ ¼å¼åŒ–æ¨¡æ¿ï¼Œæ›¿æ¢å˜é‡
        prompt = self.template
        prompt = prompt.replace('{{ documents }}', str(documents))
        prompt = prompt.replace('{{ query }}', str(query))
        return {'prompt': prompt}
# æ·»åŠ llama2å’Œmistralçš„æ¨¡æ¿é…ç½®
template_dict.update({
    'llama2': {
        'template_name': 'llama2',
        'system': None,
        'system_format': "[INST] <<SYS>>\n{content}\n<</SYS>>\n\n",
        'user_format': "{content} [/INST]",
        'assistant_format': "{content} </s>",
        'stop_word': "</s>"
    },
    'mistral': {
        'template_name': 'mistral',
        'system': None,
        'system_format': "<s>[INST] {content} [/INST]",
        'user_format': "[INST] {content} [/INST]",
        'assistant_format': "{content} </s>",
        'stop_word': "</s>"
    }
})

app = Flask(__name__)

# å…¨å±€å˜é‡
model = None
tokenizer = None
template_name = None

# RAGç›¸å…³å…¨å±€å˜é‡
rules_data = None
embeddings_model = None
vector_db = None
cached_original_prediction = None

def get_device(gpu_id=5):
    # ä½¿ç”¨æŒ‡å®šçš„GPU ID
    return torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/load_model', methods=['POST'])
def load_model():
    global model, tokenizer, template_name

    data = request.get_json()
    model_path = data.get('model_path')
    gpu_id     = data.get('gpu', 5)
    
    # ä¿å­˜GPU IDåˆ°å…¨å±€å˜é‡
    global gpu_id_global
    gpu_id_global = gpu_id
    print(f"è®¾ç½®ä½¿ç”¨GPU {gpu_id_global}")

    # æ ¡éªŒè·¯å¾„
    if not model_path or not os.path.isdir(model_path):
        return jsonify({'status': 'error', 'message': 'æ¨¡å‹è·¯å¾„æ— æ•ˆ'})

    # æ¸…ç†æ—§æ¨¡å‹
    if model is not None:
        print(f'å¼€å§‹å¸è½½æ¨¡å‹...')
        model_name = os.path.basename(model_path) if model_path else 'å½“å‰æ¨¡å‹'
        return jsonify({
            'status': 'unloading',
            'message': f'{model_name}æ­£åœ¨å¸è½½ä¸­...',
            'log': f'{model_name}æ­£åœ¨å¸è½½ä¸­...'
        })
        
        del model
        del tokenizer
        torch.cuda.empty_cache()
        print(f'{model_name}å¸è½½æˆåŠŸï¼Œæ˜¾å­˜å·²é‡Šæ”¾')
        return jsonify({
            'status': 'success',
            'message': f'{model_name}å¸è½½æˆåŠŸ',
            'log': f'{model_name}å·²æˆåŠŸå¸è½½ï¼Œæ˜¾å­˜å·²é‡Šæ”¾'
        })

    # 1. åŠ è½½ tokenizer
    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ¨¡æ¿
    if config.model_type == 'llama':
        template_name = 'llama2'
    elif config.model_type == 'mistral':
        template_name = 'mistral'
    elif hasattr(config, 'chat_template') and 'qwen' in config.chat_template.lower():
        template_name = 'qwen3'
    elif 'qwen' in os.path.basename(model_path).lower():
        template_name = 'qwen3'  # å¦‚æœæ¨¡å‹è·¯å¾„åŒ…å«qwenï¼Œä¹Ÿä½¿ç”¨qwen3æ¨¡æ¿
    else:
        template_name = 'llama2'  # é»˜è®¤ä½¿ç”¨llama2æ¨¡æ¿
    
    # llamaå’Œmistralæ¨¡å‹ä¸æ”¯æŒfast tokenizer
    use_fast = False if config.model_type in ['llama', 'mistral'] else True
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        use_fast=use_fast
    )
    
    # å¦‚æœæ˜¯Qwen3æ¨¡æ¿ï¼Œåº”ç”¨è‡ªå®šä¹‰çš„chat_template
    if template_name == 'qwen3' and 'qwen3' in template_dict:
        qwen3_template = template_dict['qwen3']
        if 'chat_template' in qwen3_template:
            tokenizer.chat_template = qwen3_template['chat_template']
            print(f"å·²åº”ç”¨Qwen3è‡ªå®šä¹‰chat_template")

    # 2. åŠ è½½æ¨¡å‹ï¼šä½¿ç”¨æŒ‡å®šçš„GPU
    device = get_device(gpu_id)
    torch.backends.cudnn.benchmark = True

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,# FP16 ç²¾åº¦
        low_cpu_mem_usage=True,   # å‡å°‘ CPU å†…å­˜å ç”¨
        trust_remote_code=True,
        device_map={"": gpu_id}  # æ˜ å°„åˆ°æŒ‡å®šçš„GPU
    )
    model.to(device)
    model.eval()
    # å¼ºåˆ¶å…¨æ¨¡å‹åŠç²¾åº¦ï¼Œé¿å…ç”Ÿæˆæ¦‚ç‡å‡ºç° NaN/Inf
    model.half()

    print(f'æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè·¯å¾„: {model_path}, è®¾å¤‡: {device}')
    return jsonify({
        'status': 'success',
        'message': f'æ¨¡å‹å·²åŠ è½½åˆ° {device}',
        'log': f'æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè·¯å¾„: {model_path}, è®¾å¤‡: {device}'
    })

def build_prompt(tokenizer, template, query, history, system=None):
    global template_name  # æ·»åŠ å…¨å±€å˜é‡å£°æ˜
    messages = []
    if system:
        messages.append({'role': 'system', 'content': str(system)})
    for h in history:
        messages.append({'role': h['role'], 'content': str(h['message'])})
    messages.append({'role': 'user', 'content': str(query)})
    
    # å¦‚æœæ˜¯Qwen3æ¨¡æ¿ä¸”tokenizeræœ‰chat_templateå±æ€§ï¼Œä½¿ç”¨åŸç”Ÿchat_template
    if template_name == 'qwen3' and hasattr(tokenizer, 'chat_template'):
        try:
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except Exception as e:
            print(f"ä½¿ç”¨Qwen3 chat_templateæ—¶å‡ºé”™: {e}")
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            prompt = ''
            for msg in messages:
                if msg['role'] == 'system' and template['system_format']:
                    prompt += template['system_format'].format(content=str(msg['content']))
                elif msg['role'] == 'user':
                    prompt += template['user_format'].format(content=str(msg['content']))
                elif msg['role'] == 'assistant':
                    prompt += template['assistant_format'].format(content=str(msg['content']))
    else:
        # åŸå§‹æ–¹æ³•æ„å»ºprompt
        prompt = ''
        for msg in messages:
            if msg['role'] == 'system' and template['system_format']:
                prompt += template['system_format'].format(content=str(msg['content']))
            elif msg['role'] == 'user':
                prompt += template['user_format'].format(content=str(msg['content']))
            elif msg['role'] == 'assistant':
                prompt += template['assistant_format'].format(content=str(msg['content']))
    
    return tokenizer(prompt, return_tensors='pt').input_ids

import csv
from werkzeug.utils import secure_filename

@app.route('/load_rules', methods=['POST'])
def load_rules():
    global rules_data, embeddings_model, vector_db
    
    data = request.get_json()
    rules_path = data.get('rules_path')
    
    if not rules_path or not os.path.isfile(rules_path):
        return jsonify({'status': 'error', 'message': 'è§„åˆ™æ–‡ä»¶è·¯å¾„æ— æ•ˆ'})
    
    try:
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        if embeddings_model is None:
            # æ·»åŠ å‚æ•°é¿å…è¿æ¥Hugging Face
            embeddings_model = SentenceTransformer(
                '/work/models/sentence-transformers/all-MiniLM-L6-v2',
                local_files_only=True,  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                use_auth_token=False    # ä¸ä½¿ç”¨auth token
            )
        
        # ä¸ºè§„åˆ™ç”Ÿæˆå‘é‡åµŒå…¥
        vector_db = []
        rules_data = []
        
        # åˆå§‹åŒ–æ–‡æ¡£åˆ†å—å™¨
        splitter = DocumentSplitter(
            split_by="word",
            split_length=300,
            split_overlap=50,
            language="zh"
        )
        splitter.warm_up()
        
        # åªæ”¯æŒCSVæ–‡ä»¶æ ¼å¼
        file_ext = os.path.splitext(rules_path)[1].lower()
        if file_ext != '.csv':
            raise ValueError(f"åªæ”¯æŒCSVæ–‡ä»¶æ ¼å¼ï¼Œä¸æ”¯æŒ{file_ext}æ ¼å¼")
        
        # å¤„ç†CSVæ–‡ä»¶
        print(f"åŠ è½½CSVæ–‡ä»¶: {rules_path}")
        documents = []
        with open(rules_path, "r", encoding="utf-8") as csvfile:
            # ä½¿ç”¨DictReaderï¼Œå®ƒä¼šå°†ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
            reader = csv.DictReader(csvfile)
            for i, row in enumerate(reader):
                # å‡è®¾CSVæœ‰'character'å’Œ'rule'åˆ—
                character = row.get('character', '').strip()
                rule = row.get('rule', '').strip()
                
                # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦ä¸ºç©º
                if not character or not rule:
                    print(f"âš ï¸ è­¦å‘Š: CSVæ–‡ä»¶ç¬¬{i+2}è¡Œç¼ºå°‘'character'æˆ–'rule'å­—æ®µï¼Œå·²è·³è¿‡ã€‚è¡Œå†…å®¹: {row}")
                    continue

                # æ„é€ æ–‡æ¡£å†…å®¹
                content = f"{character}çš„è®¾è®¡è§„åˆ™ï¼š{rule}"
                
                # æ„é€ å…ƒæ•°æ®ï¼ŒåŒ…å«CSVä¸­çš„æ‰€æœ‰åˆ—ï¼ˆæ–¹ä¾¿åç»­ä½¿ç”¨ï¼‰
                meta = {k.strip(): v.strip() for k, v in row.items() if k and v} # æ¸…ç†é”®å’Œå€¼çš„ç©ºæ ¼
                
                documents.append(Document(content=content, meta=meta))
                rules_data.append(row)

        print(f"âœ… æˆåŠŸä»CSVåŠ è½½ {len(documents)} æ¡è§„åˆ™ã€‚")
        
        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—
        if documents:
            print(f"å¼€å§‹å¯¹CSVæ–‡æ¡£è¿›è¡Œåˆ†å—ï¼ŒåŸå§‹æ–‡æ¡£æ•°: {len(documents)}")
            split_docs = []
            for doc in documents:
                # æ¨¡æ‹ŸDocumentå¯¹è±¡çš„åˆ†å—
                chunks = splitter.split_text(doc.content, chunk_size=300, chunk_overlap=50)
                for i, chunk in enumerate(chunks):
                    chunk_meta = doc.meta.copy()
                    chunk_meta['chunk_index'] = i
                    chunk_meta['original_length'] = len(doc.content)
                    split_docs.append({'text': chunk, 'meta': chunk_meta})
                
            print(f"åˆ†å—å®Œæˆï¼Œåˆ†å—åæ–‡æ¡£æ•°: {len(split_docs)}")
            
            # ä¸ºåˆ†å—åçš„æ–‡æ¡£ç”ŸæˆåµŒå…¥
            for doc in split_docs:
                embedding = embeddings_model.encode(doc['text'])
                vector_db.append({'text': doc['text'], 'embedding': embedding, 'meta': doc['meta']})
        
        print(f'æˆåŠŸåŠ è½½è§„åˆ™æ–‡ä»¶ï¼Œå…±{len(vector_db)}æ¡è§„åˆ™')
        return jsonify({
            'status': 'success',
            'message': f'æˆåŠŸåŠ è½½{len(vector_db)}æ¡è§„åˆ™',
            'log': f'è§„åˆ™æ–‡ä»¶åŠ è½½æˆåŠŸ: {rules_path}, å…±{len(vector_db)}æ¡è§„åˆ™'
        })
    except Exception as e:
        print('åŠ è½½è§„åˆ™æ–‡ä»¶æ—¶å‡ºé”™:', str(e))
        return jsonify({'status': 'error', 'message': 'åŠ è½½è§„åˆ™æ–‡ä»¶æ—¶å‡ºé”™: ' + str(e)})

@app.route('/unload_model', methods=['POST'])
def unload_model():
    global model, tokenizer, template_name, llm_generator
    
    if model is None:
        return jsonify({'status': 'error', 'message': 'æ²¡æœ‰åŠ è½½çš„æ¨¡å‹å¯å¸è½½'})
        
    try:
        model_name = os.path.basename(model_path) if 'model_path' in globals() else 'å½“å‰æ¨¡å‹'
        
        # é‡Šæ”¾æ¨¡å‹èµ„æº
        del model
        del tokenizer
        torch.cuda.empty_cache()
        model = None
        tokenizer = None
        template_name = None
        
        # æ¸…ç†LocalLLMGeneratorèµ„æº
        if llm_generator is not None:
            llm_generator.model = None
            llm_generator.tokenizer = None
            llm_generator.pipe = None
            llm_generator = None
        
        print(f'{model_name}å¸è½½æˆåŠŸï¼Œæ˜¾å­˜å·²é‡Šæ”¾')
        return jsonify({
            'status': 'success',
            'message': f'{model_name}å¸è½½æˆåŠŸ',
            'log': f'{model_name}å·²æˆåŠŸå¸è½½ï¼Œæ˜¾å­˜å·²é‡Šæ”¾'
        })
    except Exception as e:
        return jsonify({
            'status': 'error', 
            'message': f'å¸è½½è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {str(e)}'
        })




# åˆå§‹åŒ–å…¨å±€å˜é‡
llm_generator = None

@app.route('/predict', methods=['POST'])
def predict():
    global model, tokenizer, template_name, llm_generator
    if model is None or tokenizer is None:
        return jsonify({'status': 'error', 'message': 'è¯·å…ˆåŠ è½½æ¨¡å‹'})

    data = request.get_json()
    user_input = data.get('input_text', '').strip()
    print('æ”¶åˆ°é¢„æµ‹è¯·æ±‚ï¼Œè¾“å…¥å†…å®¹:', user_input)
    if not user_input:
        return jsonify({'status': 'error', 'message': 'è¯·è¾“å…¥å†…å®¹'})

    # ä½¿ç”¨æ¨¡æ¿æ„å»ºprompt
    history = data.get('history', [])
    system_prompt = data.get('system_prompt', None)
    
    # å­˜å‚¨åŸå§‹ç”¨æˆ·è¾“å…¥ç”¨äºå¯¹æ¯”
    original_user_input = user_input
    
    # æ£€ç´¢ç›¸å…³è§„åˆ™
    retrieved_rules = []
    user_input_with_context = original_user_input
    
    if vector_db and len(vector_db) > 0:
            # åˆå§‹åŒ–DocumentStoreå’ŒRetriever
            document_store = InMemoryDocumentStore()
            
            # å°†vector_dbä¸­çš„æ•°æ®æ·»åŠ åˆ°document_store
            docs_to_store = []
            for item in vector_db:
                docs_to_store.append({
                    'content': item['text'],
                    'embedding': item['embedding'],
                    'meta': item['meta']
                })
            document_store.write_documents(docs_to_store)
            
            retriever = InMemoryEmbeddingRetriever(document_store=document_store)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = embeddings_model.encode(original_user_input)
            
            # æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
            retrieval_result = retriever.run(query_embedding, top_k=3)
            top_rules = retrieval_result['documents']
            
            # ä¿å­˜æ£€ç´¢åˆ°çš„è§„åˆ™ï¼Œç¬¦åˆå‰ç«¯æ˜¾ç¤ºæ ¼å¼è¦æ±‚
            retrieved_rules = []
            for i, item in enumerate(top_rules):
                # æ„å»ºæ¯æ¡è§„åˆ™ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
                rule_info = {
                    'id': i + 1,
                    'text': item['content'], 
                    'similarity': float(item['similarity']),
                    'rank': i + 1,
                    'content': item['content'],  # ä¸ºå‰ç«¯æ˜¾ç¤ºæ·»åŠ contentå­—æ®µ
                    'score': float(item['similarity'])  # ä¸ºå‰ç«¯æ˜¾ç¤ºæ·»åŠ scoreå­—æ®µ
                }
                # å¦‚æœæœ‰å…ƒæ•°æ®ï¼Œä¹Ÿæ·»åŠ åˆ°è§„åˆ™ä¿¡æ¯ä¸­
                if 'meta' in item and item['meta']:
                    meta_info = " | ".join([f"{str(k)}: {str(v)}" for k, v in item['meta'].items()])
                    rule_info['meta'] = meta_info
                retrieved_rules.append(rule_info)
                
            # æ‰“å°æ£€ç´¢ç»“æœåˆ°é†’ç›®ç¾è§‚çš„æ¡†ä¸­ï¼Œç¬¦åˆç”¨æˆ·è¦æ±‚çš„æ ¼å¼
            print("\n" + "=" * 60)
            print("ğŸ“Œ é—®é¢˜:", original_user_input)
            print("ğŸ” æ£€ç´¢åˆ°çš„è§„åˆ™ï¼š")
            for i, rule in enumerate(retrieved_rules):
                print(f"\nğŸ’¡ è§„åˆ™ #{rule['rank']} (ç›¸ä¼¼åº¦: {rule['similarity']:.4f}):")
                print(f"{rule['text']}")
                if 'meta' in rule:
                    print(f"ğŸ“ å…ƒæ•°æ®: {rule['meta']}")
            print("=" * 60 + "\n")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨æ›´ä¸“ä¸šçš„æ¨¡æ¿
            template = """
æ ¹æ®ä»¥ä¸‹è§„åˆ™å›ç­”é—®é¢˜ï¼š

{% for doc in documents %}
- è§’è‰²: {{ doc.meta['character'] }}
  è§„åˆ™: {{ doc.content.split('ï¼š', 1)[1] if 'ï¼š' in doc.content else doc.content }} 
  {% if 'category' in doc.meta and doc.meta['category'] %}åˆ†ç±»: {{ doc.meta['category'] }} {% endif %}
{% endfor %}

é—®é¢˜ï¼š{{ query }}
è¯·æ ¹æ®ä¸Šè¿°è§„åˆ™å›ç­”é—®é¢˜ï¼Œå¦‚æœè§„åˆ™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„è§„åˆ™å›ç­”è¯¥é—®é¢˜ã€‚
"""
            
            # æ‰‹åŠ¨æ ¼å¼åŒ–æ¨¡æ¿
            formatted_rules = []
            for doc in top_rules:
                character = doc.get('meta', {}).get('character', 'Unknown')
                rule_content = doc['content'].split('ï¼š', 1)[1] if 'ï¼š' in doc['content'] else doc['content']
                category = doc.get('meta', {}).get('category', '')
                
                rule_str = f"- è§’è‰²: {character}\n  è§„åˆ™: {rule_content}"
                if category:
                    rule_str += f"\n  åˆ†ç±»: {category}"
                formatted_rules.append(rule_str)
            
            formatted_rules_str = "\n".join(formatted_rules)
            user_input_with_context = f"æ ¹æ®ä»¥ä¸‹è§„åˆ™å›ç­”é—®é¢˜ï¼š\n\n{formatted_rules_str}\n\né—®é¢˜ï¼š{original_user_input}\nè¯·æ ¹æ®ä¸Šè¿°è§„åˆ™å›ç­”é—®é¢˜ï¼Œå¦‚æœè§„åˆ™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•æ ¹æ®æä¾›çš„è§„åˆ™å›ç­”è¯¥é—®é¢˜ã€‚"
            
            # æ‰“å°æ£€ç´¢ç»“æœåˆ°é†’ç›®ç¾è§‚çš„æ¡†ä¸­
            print("\n" + "=" * 80)
            print("ğŸ” æ•°æ®åº“æ£€ç´¢å†…å®¹")
            print("=" * 80)
            for i, item in enumerate(top_rules):
                char = item.get('meta', {}).get('character', 'Unknown')
                # æå–è§„åˆ™éƒ¨åˆ†ç”¨äºæ˜¾ç¤º
                rule_part = item['content'].split('ï¼š', 1)[1] if 'ï¼š' in item['content'] else item['content']
                print(f"\nğŸ’¡ æ£€ç´¢ç»“æœ #{i+1} (ç›¸ä¼¼åº¦: {item['similarity']:.4f}):")
                print(f"  è§’è‰²: {char}")
                print(f"  è§„åˆ™: {rule_part[:200]}...")
                # å¯ä»¥æ‰“å°å…¶ä»–å…ƒæ•°æ®
                if 'category' in item.get('meta', {}):
                    print(f"  åˆ†ç±»: {item['meta']['category']}")
            print("=" * 80 + "\n")
    
    try:
        # åˆå§‹åŒ–LocalLLMGenerator
        if llm_generator is None:
            print("åˆå§‹åŒ–LocalLLMGenerator...")
            # ä»å…¨å±€å˜é‡è·å–æ¨¡å‹è·¯å¾„
            model_path_val = globals().get('model_path', '')
            # ä»å…¨å±€å˜é‡è·å–GPU IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼5
            gpu_id_val = globals().get('gpu_id_global', 5)
            device = f'cuda:{gpu_id_val}' if torch.cuda.is_available() else 'cpu'
            llm_generator = LocalLLMGenerator(model_path=model_path_val, device=device)
            # ä¼ é€’tokenizerï¼Œé¿å…é‡å¤åŠ è½½
            llm_generator.tokenizer = tokenizer
            llm_generator.model = model
            # å½“æ¨¡å‹é€šè¿‡accelerateåŠ è½½æ—¶ï¼Œä¸éœ€è¦åœ¨pipelineä¸­æŒ‡å®šdeviceå‚æ•°
            llm_generator.pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer
            )
            llm_generator._is_warmed_up = True
            print("âœ… LocalLLMGeneratoråˆå§‹åŒ–å®Œæˆ")
        
        global cached_original_prediction
        prediction_original = None
        prediction_with_context = ""
        
        # å§‹ç»ˆç¡®ä¿without ragæ¡†æœ‰å†…å®¹ï¼ˆæ¨¡å‹å›ç­”ï¼‰
        if not cached_original_prediction:
            print("ğŸ”„ ç”ŸæˆåŸå§‹å›ç­”å¹¶ç¼“å­˜ï¼ˆwithout ragæ¡†å†…å®¹ï¼‰")
            response_original = llm_generator.run(prompt=original_user_input)
            cached_original_prediction = response_original["replies"][0]
        prediction_original = cached_original_prediction
        
        # åªæœ‰åœ¨åŠ è½½äº†è§„åˆ™æ–‡ä»¶æ—¶ï¼Œæ‰ç”Ÿæˆå¸¦è§„åˆ™çš„å›ç­”
        if vector_db and len(vector_db) > 0:
            # å·²åŠ è½½è§„åˆ™çš„æƒ…å†µï¼šåœ¨with ragæ¡†æ˜¾ç¤ºç»“åˆè§„åˆ™çš„å›ç­”
            print("ğŸ”„ å·²åŠ è½½è§„åˆ™æ–‡ä»¶ï¼Œç”Ÿæˆå¸¦è§„åˆ™çš„å›ç­”ï¼ˆwith ragæ¡†å†…å®¹ï¼‰...")
            response_with_context = llm_generator.run(prompt=user_input_with_context)
            prediction_with_context = response_with_context["replies"][0]
        else:
            # æœªåŠ è½½è§„åˆ™çš„æƒ…å†µï¼šwith ragæ¡†æ˜¾ç¤ºç©ºå­—ç¬¦ä¸²ï¼ˆä»€ä¹ˆéƒ½æ²¡æœ‰ï¼‰
            print("ğŸ”„ æœªåŠ è½½è§„åˆ™æ–‡ä»¶ï¼Œwith ragæ¡†å†…å®¹ä¸ºç©º")
            prediction_with_context = ""
        
        print(f'ç”Ÿæˆå®Œæˆï¼Œè¾“å…¥: {original_user_input}, å¸¦è§„åˆ™è¾“å‡º: {str(prediction_with_context)[:50]}...')
    except Exception as e:
        print('ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸:', str(e))
        return jsonify({'status': 'error', 'message': 'ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸:' + str(e)})

    # ç¡®ä¿è¿”å›ç»“æœä¸­åŒ…å«åŸå§‹å›ç­”ï¼Œæ— è®ºæ˜¯å¦åŠ è½½è§„åˆ™
    if prediction_original:
        # ç¡®å®šä½¿ç”¨å“ªä¸ªå›ç­”æ›´æ–°å†å²è®°å½•
        if prediction_with_context and vector_db and len(vector_db) > 0:
            # æœ‰è§„åˆ™æ–‡ä»¶ä¸”ç”Ÿæˆäº†å¸¦è§„åˆ™çš„å›ç­”
            history_answer = prediction_with_context
            log_content = f'è¾“å…¥: {original_user_input}\nå¸¦è§„åˆ™è¾“å‡º: {str(prediction_with_context)}\nåŸå§‹è¾“å‡º: {str(prediction_original)}'
        else:
            # æ²¡æœ‰è§„åˆ™æ–‡ä»¶æˆ–å¸¦è§„åˆ™çš„å›ç­”ä¸ºç©º
            history_answer = prediction_original
            log_content = f'è¾“å…¥: {original_user_input}\nåŸå§‹è¾“å‡º: {str(prediction_original)}'
        
        # æ›´æ–°å†å²è®°å½•
        updated_history = copy.deepcopy(history)
        updated_history.append({'role': 'user', 'message': original_user_input})
        updated_history.append({'role': 'assistant', 'message': history_answer})
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            'status': 'success', 
            'prediction': prediction_with_context,  # å¸¦è§„åˆ™çš„å›ç­”
            'original_prediction': prediction_original,  # ä¸å¸¦è§„åˆ™çš„å›ç­”
            'history': updated_history,
            'retrieved_rules': retrieved_rules,  # åªæœ‰åœ¨å·²åŠ è½½è§„åˆ™æ—¶æ‰ä¼šæœ‰å†…å®¹
            'log': log_content
        }
        return jsonify(result)
    else:
        return jsonify({'status':'error','message':'ç”Ÿæˆå†…å®¹ä¸ºç©º'})
        


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
    