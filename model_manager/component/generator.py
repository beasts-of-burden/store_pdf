import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from typing import List

class LocalLLMGenerator:
    def __init__(self, model_path: str, device: str = "cuda"):
        self.model_path = model_path
        self.device = device
        self.tokenizer = None
        self.model = None
        self.pipe = None
        self._is_warmed_up = False
        
        # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
        if device == "cuda" and not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
            self.device = "cpu"
            
    def _load_model(self):
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True  # å¿…é¡»å¯ç”¨ä»¥æ”¯æŒQwen3ç­‰æ¨¡å‹çš„ç‰¹æ®Štoken
        )
        
        # å°è¯•ä»deviceå­—ç¬¦ä¸²ä¸­æå–GPU IDï¼Œå¦‚æœæ˜¯cuda:Xæ ¼å¼
        gpu_id = 0  # é»˜è®¤å€¼
        if self.device.startswith('cuda:'):
            try:
                gpu_id = int(self.device.split(':')[1])
            except (IndexError, ValueError):
                pass
        
        # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„GPU ID
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map={"": gpu_id} if torch.cuda.is_available() else "auto",  # æ˜ å°„åˆ°æŒ‡å®šçš„GPU
            trust_remote_code=True  # âœ… å¿…é¡»å¯ç”¨
        )
        
        # å½“æ¨¡å‹é€šè¿‡accelerateåŠ è½½æ—¶ï¼Œä¸éœ€è¦åœ¨pipelineä¸­æŒ‡å®šdeviceå‚æ•°
        self.pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer
        )
        
        self._is_warmed_up = True
        print("âœ… Local LLM Generator ready.")
    
    def run(self, prompt: str):
        if not self._is_warmed_up:
            self._load_model()

        print(f"ğŸ¯ æ”¶åˆ°æç¤ºè¯: {str(prompt)[:100]}...")

        # âœ… æ„é€ ç¬¦åˆ Qwen3 æ¨¡æ¿çš„æ¶ˆæ¯ç»“æ„
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè§’è‰²è®¾è®¡åˆè§„åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„è§„åˆ™å›ç­”é—®é¢˜ã€‚"},
            {"role": "user", "content": str(prompt)}  # ç¡®ä¿promptæ˜¯å­—ç¬¦ä¸²
        ]

        # âœ… ç›´æ¥ä½¿ç”¨æ‰‹åŠ¨æ„å»ºçš„æ¨¡æ¿æ ¼å¼ï¼Œé¿å…tokenizer.apply_chat_templateå¯èƒ½å¸¦æ¥çš„é—®é¢˜
        print(f"ğŸ”„ ç›´æ¥ä½¿ç”¨æ‰‹åŠ¨æ„å»ºçš„æ¨¡æ¿æ ¼å¼ï¼Œè·³è¿‡tokenizer.apply_chat_template")
        
        # å¯¹äºQwen2æ¨¡å‹ï¼Œä½¿ç”¨å…¼å®¹çš„æ¨¡æ¿æ ¼å¼
        formatted_prompt = "<|im_start|>system\n" + messages[0]["content"] + "<|im_end|>\n<|im_start|>user\n" + messages[1]["content"] + "<|im_end|>\n<|im_start|>assistant\n"

        # âœ… è°ƒè¯•æ‰“å°ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
        print("ğŸ“ Prompt sent to model:")
        try:
            # ç¡®ä¿formatted_promptæ˜¯å­—ç¬¦ä¸²æ ¼å¼
            print(str(formatted_prompt))
        except Exception as e:
            print(f"æ‰“å°promptæ—¶å‡ºé”™: {e}")
        print("-" * 50)

        outputs = self.pipe(
            formatted_prompt,
            max_new_tokens=1024,  # å¢åŠ tokené™åˆ¶åˆ°1024
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.1,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id,
            return_full_text=False  # âœ… ä¸è¿”å›è¾“å…¥éƒ¨åˆ†
        )

        generated_text = outputs[0]["generated_text"].strip()
        return {"replies": [generated_text]}